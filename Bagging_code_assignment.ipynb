{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding assignment: Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Ensemble methods and Bagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Bagging is an <span style='background :yellow' > ensemble method </span>used in machine learning that can be applied to almost any <span style='background :yellow' >base learner </span> model. Generally speaking, it <span style='background :yellow' > lowers the variance of the model while maintains the low bias of the model</span>. <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; You may see a lot of new terminologies here that seems overwhelming. No worries, we will go over these concepts one by one. So far all you need to know it that bagging helps us develop a more reliable model. In this code assignment we will be starting from scratch and build up these concepts with yout knowledge in EE16A EE16B and CS61B.  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Here is the <span style='background :yellow' > outline</span> of the assignment. If you are already familiar with some concept, you can feel free to skim through the part.  \n",
    "1. Machine learning Basics recap <br>\n",
    "    1.1 Linear regression <br>\n",
    "    1.2 Polynomial regression <br>\n",
    "    1.3 Overfitting<br>\n",
    "2. Bias and Variance <br>\n",
    "    2.1 Bagging: an experiment <br>\n",
    "    2.2 Bagging theory <br>\n",
    "3. Bootstrap <br>\n",
    "4. K-fold cross-validation <br>\n",
    "5. Decision Tree <br>\n",
    "    5.1 Overfitting <br>\n",
    "    5.2 Bagged trees <br>\n",
    "    5.3 Random forest <br>\n",
    "    \n",
    "Here we want you to assess your familiarity with EE16A/B ML. If you feel confident doing linear and polynomial regression and are familiar with ideas of overfitting, you can skim through part A. If you want to have a solid background before going further, take the time to go through these code work.  \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Here are some useful machine learning libraries we want you to import. <span style='background :yellow' >Sklearn </span> for machine learning deploymeny, <span style='background :yellow' >Numpy </span> (often seen as np.xxx ) for efficient matrix related calculation, <span style='background :yellow' >Matplotlib\n",
    "</span>(often seen as plt.xxx) for visualization. If you want to get deeper in ML, better get familiar with these libraries :))\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm as svm\n",
    "import sklearn.datasets as dt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import sys\n",
    "#!{sys.executable} -m pip install mlxtend\n",
    "#Uncomment the line above if you do not have mlxtend installed\n",
    "#change mlxtend to any other packages you do not have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 EE16A/B Machine learning RECAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "\n",
    "In EE16A, we have learned about the basics of machine learning: identifying the problem of classification, estimation, prediction and clustering, mastering some linear algebra techniques to solve machine learning problem, eg. least square, optimizing a loss function. (vFor a quick 16A ML recap please go to https://inst.eecs.berkeley.edu/~ee16a/fa19/lecture/2019-11-12_11A.pdf ) Let's start with a set of problem that we are all familiar with in EE16A, \"the line of best fit\" problems. \n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex.1 Linear regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple exercise, we will be dealing with a toy example that helps you recap the setting of linear regression. You will also be able to bridge the gap between linear algebra and the larger setting of machine learning problem. Let's say that we are given a set of peerfectly linearly correlated data and we would like to figure out the exact formulation of their relations. We learned in the EE16A that we could formulate the problem as a least square problem and find its solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Here we create a set of linearly correlated points: wx + b = y. Here w and b are unknown.\n",
    "\"\"\"\n",
    "w = 1.5\n",
    "b = 30\n",
    "xs, ys = [], []\n",
    "for i in range(100):\n",
    "    x = 100 * random.random() - 50\n",
    "    xs.append(x)\n",
    "    ys.append(w * x + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(xs,ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we know $X$ and $y$ . We can then formulate the problem as $$Ax = b$$  \n",
    "$$A = \\begin{bmatrix}\n",
    "x_1 & 1 \\\\\n",
    "x_2 & 1 \\\\\n",
    "x_3 & 1 \\\\\n",
    "\\vdots  & \\vdots \\\\\n",
    "x_m & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "$$b = \\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots\\\\\n",
    "y_m\n",
    "\\end{bmatrix}$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Q1:</b> Derive the algebric solution to the Least square problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Algebra way of solving it comes by matrix calculation\n",
    "\n",
    "X_s = np.vstack([xs, np.ones(len(xs))]).T\n",
    "w, b = #TODO: Calculate w, b using least square formula\n",
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Q2:</b> Use Numpy's Least square method to code up the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s = np.vstack([xs, np.ones(len(xs))]).T\n",
    "#TODO: Use numpy's least square method. print the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these calculation we are able to obtain the predicted $Y$<sub>pred</sub> for any given $x$. By far you should be familiar with the pipeline of a basic machine learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex.2. Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous example is set the stage for a larger set of regression problem called polynomial regression. Here we will give you some example to understand the set of regression problem better. Say our ground truth model is a second degree polynomial. If we used linear regression to model the problem, what problem will we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: blue; line-height:1.5\">\n",
    "Your observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The ground truth data is modeled by a second degree polynomial. y = x**2 + 3x +2\n",
    "\n",
    "x_s, y_s = [], []\n",
    "for i in range(100):\n",
    "    x = 3 * random.random() - 3\n",
    "    x_s.append(x)\n",
    "    y_s.append(x**2 + 3*x +2)\n",
    "x_s = np.array(x_s)\n",
    "plt.scatter(x_s,y_s)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us use linear method to model this polynomial data. Plot the line you get on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you are required to the code up function plot_best_fit_poly,\n",
    "# which takes in data and the desired degree of polynomial approximation.\n",
    "#It should plot the data along with a the best fit polynomial.\n",
    "\n",
    "def plot_best_fit_poly(x_s, y_s,deg):\n",
    "    # TODO: Calculate the coefficients here\n",
    "    # Hint: use np.polyfit here\n",
    "    poly = np.poly1d(coefficients)\n",
    "    new_x = np.linspace(min(x_s), max(x_s))\n",
    "    new_y = poly(new_x)\n",
    "    plt.figure()\n",
    "    plt.plot(x_s, y_s,\"o\", new_x, new_y, linewidth = 3)\n",
    "    plt.legend(('polynomial data', str(deg)+'th degree approximation'),\n",
    "           loc='upper right')\n",
    "    plt.show()\n",
    "    print('this is best fitted polynomial for degree' + str(deg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Here you will derive the line of best using least square\n",
    "# You are allowed to use library functions\n",
    "X_s = np.vstack([x_s, np.ones(len(x_s))]).T\n",
    "w, b = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = np.linspace(min(x_s), max(x_s))\n",
    "new_y = w * new_x + b\n",
    "plt.plot( x_s,y_s, 'o', new_x, new_y, linewidth = 3)\n",
    "plt.legend(('polynomial data', 'Linear approximation'),\n",
    "           loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy method polyfit will help you find the coefficient of polynomial given all the points on the curve. Please derive the most fitted the polynomial and plot it on top of the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_best_fit_poly(x_s, y_s,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try using polynial of degree 3,5,10 to model the second degree polynomial example. What do you find?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "Comment on your finding\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: blue; line-height:1.5\">\n",
    "Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [3,5,10]\n",
    "for num in nums:\n",
    "    plot_best_fit_poly(x_s, y_s,num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use degree 3,5,100, we will get the same result! Since we are ust putting all the terms over 2 degree to zero. Now it seems that there is an advantage to use higher degree polynomials, since it is more expressive and coudl model all sorts of functions. Is this the real case? Let's go into the next section and examine the case when we have imperfect data from real life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex.3. Error term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real life, it is almost impossible to get a set of perfect gauched data. There is a thousand ways to get things wrong. In this part, let jump out of the perfect examples and step our foot in the real life scenarios with error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We manually plant some noises in the data, an error term that N: (0,0.5)\n",
    "\n",
    "errors = []\n",
    "for i in range(100):\n",
    "    errors.append(random.gauss(0,0.5))\n",
    "y_with_error = np.add(y_s,errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare the ground truth and the classifier we get with the noisy data\n",
    "for i in range(1,17,2):\n",
    "    plot_best_fit_poly(x_s, y_with_error,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, as we increase the polynomial term, the decision boundry becomes spiky. We relate this observation to the idea of overfitting: using a too complicated model to capture the nitty gritty of the data while not being able to generalize to other data. We often tie the idea of overfitting to high variance. If you select a bunch of samples from a population repeatedly whiling use an overfitting model, chances are that all the models you get each time will cater to the selected population and will vary to each other a lot. This is what we mean by high variance. In the next section, we will be exploring the bias variance trade off empirically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance and Bias: a brief recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Week 2, we have learned about bias-variance decomposition. We learned about varince bias decomposition which consists of three terms: bias of the method, variance of the method and irreducible error. Let's first do a basic exericse to recap the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ground Truth Function y = x + sin(1.5x), observed data y_obs = x + sin(1.5) + N(0,1)\n",
    "x_obs, y_obs = [],[]\n",
    "for i in range(100):\n",
    "    x = random.random() * 4 - 2\n",
    "    x_obs.append(x)\n",
    "    y_obs.append(x+ np.sin(1.5*x)+random.gauss(0,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = np.linspace(min(x_obs), max(x_obs))\n",
    "new_y = new_x + np.sin(1.5 * new_x)\n",
    "plt.plot(x_obs, y_obs,'o',new_x, new_y, linewidth = 3)\n",
    "plt.legend((\"noisy data\",\"ground truth value\"), loc = \"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get 20 separate observation dataset from the underlying function. Notice that we have a gaussian error term $N~(0,0.5)$ that will make each dataset different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(num):\n",
    "    x_data,y_data = [],[]\n",
    "    for i in range(num):\n",
    "        x_obs, y_obs = [],[]\n",
    "        for i in range(100):\n",
    "            x = random.random() * 4 - 2\n",
    "            x_obs.append(x)\n",
    "            y_obs.append(x+ np.sin(1.5*x)+random.gauss(0,0.5))\n",
    "        x_data.append(x_obs)\n",
    "        y_data.append(y_obs)\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bundle, y_bundle = gen_data(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:Simulate the data with degree 1-4 polynomial and report the variance and bias of the result\n",
    "\n",
    "def bias_variance_demo(degree):\n",
    "    coeffs = []\n",
    "    for i in range(20):\n",
    "        x_s = x_bundle[i]\n",
    "        y_s = y_bundle[i]\n",
    "        #TODO calculate the coefficients and set new_x1 and new_y1 to things you want to plot\n",
    "        plt.plot(new_x1, new_y1, linewidth = 0.5)\n",
    "    mean_coeff = np.mean((coeffs),axis = 0)\n",
    "    mean_y = np.poly1d(mean_coeff)(new_x)\n",
    "    plt.plot(new_x, mean_y, linewidth = 2, label = 'approximation')\n",
    "    plt.plot(new_x, new_y, linewidth = 2, label = 'ground truth')\n",
    "    plt.legend(loc = 'upper left')\n",
    "    plt.title(str(degree) + \"th degree polynomial simulation of the function x+ sin(1.5*x)\")\n",
    "    plt.show()\n",
    "\n",
    "    bias = #TODO\n",
    "    y_s = []\n",
    "    for i in range(20):\n",
    "        coeff = coeffs[i]\n",
    "        poly = np.poly1d(coeff)\n",
    "        y_s.append(poly(new_x))    \n",
    "    variance = #TODO\n",
    "    print(\"bias is \" + str(bias))\n",
    "    print(\"variance is \" + str(variance))\n",
    "    print(\"bias**2 + variance = \" + str(bias**2+variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for deg in range(1,10):\n",
    "    bias_variance_demo(deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One would observe that as the degree of polynomial increases the bias is steadily decreasing yet the variance is increasing. Why is that? You should already encounter the concept of bias variance decomposition in previous weeks and in notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging: An experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, different learners and model classes have different tradeoffs.  \n",
    "\n",
    "– large bias/small variance: few features, highly regularized, highly pruned decision trees, large-k kNN etc  \n",
    "– small bias/high variance: many features, less regularization, small-k k-NN etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural questions arises as well deal with models with low bias and high variance. How can we prevent them from overfitting? From the previous part, we find that the mean model is much mroe smoothened out. From there, one natural intuition arises: averaging out the result of the model. In the previous example, we had 20 datasets generated from the underlying ground truth function. In real life, we only have one set of dataset and we do not have the gournd truth function. How could we generate all the datasets? We used the idea of bootstrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate noisy x and y data\n",
    "x_noisy, y_noisy = gen_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate standard test data \n",
    "x_test, y_test = [], []\n",
    "x_min, x_max = min(x_noisy[0]), max(x_noisy[0])\n",
    "for i in range(30):\n",
    "    single_x = random.random()*(x_max-x_min)+x_min\n",
    "    x_test.append(single_x)\n",
    "    y_test.append(single_x+ np.sin(1.5*single_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:Code up the function that computes regression error for bagging learner\n",
    "def bagging_poly_error(x_train,y_train,degree, iteration,x_test, y_test):\n",
    "    y_pred = []\n",
    "    for i in range(iteration):\n",
    "        #compute poly error for each sample, use random.sample for getting datasets\n",
    "    return np.abs(np.sum(np.mean(y_pred, axis = 0) - y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_errors = []\n",
    "for i in range(2,20,2):\n",
    "    bagging_error = []\n",
    "    for i in range (1,30,3):\n",
    "        bagging_error.append(bagging_poly_error(x_noisy[0],y_noisy[0],16,i,x_test, y_test))\n",
    "    bagging_errors.append(bagging_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Code up the function that computes regression error for polynomial learner\n",
    "def poly_error(x_train,y_train,degree, x_test, y_test):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.figure()\n",
    "    plt.plot(bagging_errors[i],label = 'bagging error')\n",
    "    plt.plot( np.ones(10)*regression_errors[i],label = 'regression error')\n",
    "    plt.legend(loc = 'upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on what you observe.\n",
    "You can see there is almost always certain times that bagging error is lower than regression error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: blue; line-height:1.5\">\n",
    "Your observation here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/700/1*_pfQ7Xf-BAwfQXtaBbNTEg.png\">\n",
    "From the previous example, we can see that bagging can somehow avoid overfitting and get some pretty good result. From a theoretic pespective how does it really work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do bagging, we first bootstrap M different samples without replacement from the population. Since we are sampling without replacement, then these samples are independent of each other. Thus we name them as $Y_1, Y_2, ..., Y_m$ independent random variables each with mean $\\mu$ and variance $\\sigma ^2$. As we average them, we are basically taking the expectation of the following terms which you will see is still $\\mu$ $$ \\frac{1}{M}  \\sum_{i=1}^{M} Y_i = \\frac{1}{M} \\times {M \\mu} = \\mu $$ Therefore after all these mumble jumbles we still get that good low bias. As for variance, let's do our calculation: $$ var(\\frac{1}{M}\\sum_{i=1}^{M} Y_i) = \\frac{1}{M^2} \\times var(\\sum_{i=1}^{M} Y_i) = \\frac{1}{M^2} \\times \\sigma^2 \\times M  = \\frac{\\sigma^2} {M} $$ As we can see the variance goes down linearly as M increases. This is the exact reason why we are able to get a better result in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, observe the following results of applying bagging to two exising dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mlxtend.data import iris_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = iris_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=y)\n",
    "\n",
    "\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=123)\n",
    "\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        tree, X_train, y_train, X_test, y_test, \n",
    "        loss='0-1_loss',\n",
    "        random_seed=123)\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import bias_variance_decomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=123)\n",
    "bag = BaggingClassifier(base_estimator=tree,\n",
    "                        n_estimators=100,\n",
    "                        random_state=123)\n",
    "\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        bag, X_train, y_train, X_test, y_test, \n",
    "        loss='0-1_loss',\n",
    "        random_seed=123)\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from mlxtend.data import boston_housing_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = boston_housing_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "tree = DecisionTreeRegressor(random_state=123)\n",
    "\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        tree, X_train, y_train, X_test, y_test, \n",
    "        loss='mse',\n",
    "        random_seed=123)\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(random_state=123)\n",
    "bag = BaggingRegressor(base_estimator=tree,\n",
    "                       n_estimators=100,\n",
    "                       random_state=123)\n",
    "\n",
    "avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        bag, X_train, y_train, X_test, y_test, \n",
    "        loss='mse',\n",
    "        random_seed=123)\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Feature from the original csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm as svm\n",
    "import sklearn.datasets as dt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Admission_Predict.csv', index_col='Serial No.')\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Chance of Admit ', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = df['Chance of Admit '].values\n",
    "y_val[y_val>0.8] = 1\n",
    "y_val[y_val != 1] = 0\n",
    "df['Chance of Admit '] = y_val\n",
    "y = df['Chance of Admit ']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap\n",
    "If we randomly select some observations from dataset and use the sample to estimate some unknown value we want only once, the value cannot represent the real one. Instead, we can select sample multiple time, and average the value we found each time.It will make our prediction closer to the real value. That is the idea of Bootstrap. It is robustness and high efficiency because we do not need to add additional data.\n",
    "Bootstrap can also be used to create randomness in data.\n",
    "<img src=\"https://habrastorage.org/webt/n0/dg/du/n0dgduav1ygc3iylumtwjcn15mu.png\">\n",
    "\n",
    "*Cite: Image from https://www.kaggle.com/kashnitsky/topic-5-ensembles-part-1-bagging*\n",
    "Your task for this part is to use Bootstraping to find the average GRE score for students with `Chance of Admit` > 0.8.\n",
    "\n",
    "a) Before we actually start, let try to find the mean of `GRE Score` in this whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_mean = #TODO : find the mean of GRE Score in this whole dataset.\n",
    "print(real_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Select 320 students randomly without replacement, and store the content in the variable called `sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = #TODO: sample students without replacement \n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) In `sample` dataframe, find the average of GRE Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onetime_mean = #TODO: take mean of sample\n",
    "onetime_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Part (b) and (c) show the process of sampleing and finding the mean of sampled `GRE Score`. Now complete bootstrap function, so that we can run this processes multiple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(dataframe, n):\n",
    "    \n",
    "    avg = []\n",
    "    for i in np.arange(n):\n",
    "        #TODO finish the for loop\n",
    "        bootstrap_sample = \n",
    "        new_avg = \n",
    "        avg = \n",
    "            \n",
    "    mean = #TODO: np.mean(avg)\n",
    "    return mean    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_mean = bootstrap(df, 300)\n",
    "print(\"By using Bootstrap with 300 times sampling, the mean of GRE Score is\", bootstrap_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with `onetime_mean`, `bootstrap_mean`, which one is closer to the `real_mean`? What can you conclude?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: blue; line-height:1.5\">\n",
    "Your observation here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross-Validation\n",
    "In the case of insufficient dataset, we can use K-Fold Cross-Validation to maximize the use of the dataset. \n",
    "\n",
    "As you can see in the following image, after separate the dataset into training set and test set, we initialize k, which is the number of folds that we want to separate in the training set. Then we separate the training set into k equally sized subset by using KFold() function. Then we can train our model on each split, and get the corresponding error rate each time. Finally, we average the error, and use it as an estimate of the accuracy of the model algorithm.\n",
    "\n",
    "<img src=\"https://www.textbook.ds100.org/_images/bias_cv_5_fold_cv.jpg\">\n",
    "\n",
    "*Cite: Image from:https://www.textbook.ds100.org/ch/15/bias_cv.html?highlight=cross#k-fold-cross-validation*\n",
    "In this part, we will use linear regression model to predicts TOEFL Score from GRE Score.\n",
    "\n",
    "a)First, draw a scatter plot between `df['GRE Score']` and `df['TOEFL Score']`. Also set the title and axises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: whole part, Hint: look up plt.scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Use train_test_split separate into training set and test set, with test_size = 0.25.\n",
    "   Then split the dataframe into 5 part, i.e. `numsplit` = 5, and compute `k_fold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import KFold\n",
    "#TODO: Split the training data and test data. Hint: look up the function imported\n",
    "X_tr, X_te, Y_tr, Y_te = #\n",
    "numsplit = #\n",
    "k_fold = #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Build linear regression model and finish `rmse` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "model = #TODO: Pick linear regression model from linear_model\n",
    "\n",
    "def rmse(y_real, y_predict):\n",
    "   #TODO: return the squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Finish `rmse_kfold` function, and print the average rmse error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_kfold(X_tr):\n",
    "    err = []\n",
    "    for tr, va in k_fold.split(X_tr):\n",
    "        #TODO:\n",
    "        sub_x_tr, sub_x_va = X_tr.iloc[tr], X_tr.iloc[va]\n",
    "        sub_y_tr, sub_y_va = Y_tr.iloc[tr], Y_tr.iloc[va]\n",
    "        model.fit(sub_x_tr,sub_y_tr)\n",
    "\n",
    "    train_err = #TODO: Use function you wrote above\n",
    "    err.append(train_err)\n",
    "    return np.mean(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_err = rmse_kfold(X_tr)\n",
    "print(\"The rmse error by using K-Fold is\", rmse_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "Now let's briefly introduce you to decision tree. Tree-based algorithms are a popular family of related non-parametric and supervised methods for both classification and regression. If you're wondering what supervised learning is, it's the type of machine learning algorithms which involve training models with data that has both input and output labels (in other words, we have data for which we know the true class or values, and can tell the algorithm what these are if it predicts incorrectly).\n",
    "\n",
    "The decision tree looks like a vague upside-down tree with a decision rule at the root, from which subsequent decision rules spread out below. \n",
    "\n",
    "Suppose we have four objects to classify: hawk, penguin, dolphin, and bear. It turns out that we can use a decision tree to correctly classify these four animals based on three features: \"has feathers?\", \"Can fly\", and \"Has finns\"\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/824/0*J2l5dvJ2jqRwGDfG.png\">\n",
    "\n",
    "A numerical representation of decision tree will be a threshold of a specific feature. Let's look at our data as an example. We limit our decision tree depth to 3 to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sklearn.tree\n",
    "dt = DecisionTreeClassifier(max_depth = 3)\n",
    "predY = dt.fit(X_tr, Y_tr).predict(X_te)\n",
    "predY[predY >= 0.5] = 1\n",
    "predY[predY != 1] = 0\n",
    "print(\"Accuracy of a single decision tree:\" + str(accuracy_score(Y_te, predY)))\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (3,3), dpi=300)\n",
    "axes.set_title('College Admission Decision Tree')\n",
    "_ = tree.plot_tree(dt, \n",
    "                   feature_names=X.columns,  \n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that GPA is still the most important feature for graduate school admission, followed by TOEFL and GRE. Go bears!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "Overfitting: Or Why a Forest is better than One Tree\n",
    "The reason the decision tree is prone to overfitting when we don’t limit the maximum depth is because it has unlimited flexibility, meaning that it can keep growing until it has exactly one leaf node for every single observation, perfectly classifying all of them. But we have reduced the variance of the decision tree but at the cost of increasing the bias, resulting in a lower test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth = 100) #TODO: change max_depth here\n",
    "dt = dt.fit(X_tr, Y_tr)\n",
    "predY_tr = dt.predict(X_tr)\n",
    "predY_tr[predY_tr >= 0.5] = 1\n",
    "predY_tr[predY_tr != 1] = 0\n",
    "predY = dt.predict(X_te)\n",
    "predY[predY >= 0.5] = 1\n",
    "predY[predY != 1] = 0\n",
    "print(\"Training accuracy:\" + str(accuracy_score(Y_tr, predY_tr)))\n",
    "print(\"Testing accuracy:\" + str(accuracy_score(Y_te, predY)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Try out on the different max depth of decision tree and report when the accuracy reaches 1 for training data and when the test accuracy reaches maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: blue; line-height:1.5\">\n",
    "Your observation here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging them together\n",
    "Intuitively, a single tree may not be clever enough to classify our master school students. Now our goal is to implement a model that bags results of multiple decision trees and explore whether bagging is actually going to improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "class BaggedTrees:\n",
    "\n",
    "    def __init__(self, params=None, n=200):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        self.params = params\n",
    "        self.n = n\n",
    "        self.decision_trees = [\n",
    "            DecisionTreeClassifier(max_depth = 3, random_state = i, **self.params)\n",
    "            for i in range(self.n)\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n):\n",
    "            #change the following line with bootstrapping\n",
    "            #You may want to change this line\n",
    "            self.decision_trees[i].fit(X_tr, Y_tr)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        yhat = [self.decision_trees[i].predict(X) for i in range(self.n)]\n",
    "        # TODO: compute yhat_avg for BaggedTrees\n",
    "        # HINT: take the average and predict results, note that our prediction only consists of 0 and 1\n",
    "        return yhat_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt = BaggedTrees()\n",
    "predY = bt.fit(X_tr, Y_tr).predict(X_te)\n",
    "predY[predY >= 0.5] = 1\n",
    "predY[predY != 1] = 0\n",
    "print(accuracy_score(Y_te, predY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the accuracy of testing data is getting better after averaging the result from 100 decision trees or any number of decision trees (change n)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "You just implemented a random forest by yourself! Congratulations. Random Forest is a tree-based machine learning algorithm that leverages the power of multiple decision trees for making decisions. As the name suggests, it is a “forest” of trees!\n",
    "\n",
    "But why do we call it a “random” forest? That’s because it is a forest of randomly created decision trees. As you can see from the skeleton code above -- we set the random state for each decision tree and thus each node in the decision tree works on a random subset of features to calculate the output. The random forest then combines the output of individual decision trees to generate the final output.\n",
    "\n",
    "And that is our bagged trees. Let's see the result of random forest implemented by sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(max_depth = 3,n_estimators = 100,random_state=1)\n",
    "predY = rf.fit(X_tr, Y_tr).predict(X_te)\n",
    "predY[predY >= 0.5] = 1\n",
    "predY[predY != 1] = 0\n",
    "print(accuracy_score(Y_te, predY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://miro.medium.com/max/500/1*10t9S7xvWE5Z3NEZrmHG2w.jpeg\">\n",
    "<center>This is bagging! Try to get the intuition here.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "https://blog.paperspace.com/decision-trees/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://becominghuman.ai/ensemble-learning-bagging-and-boosting-d20f38be9b1e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/kashnitsky/topic-5-ensembles-part-1-bagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.textbook.ds100.org/ch/15/bias_cv.html?highlight=cross#k-fold-cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
