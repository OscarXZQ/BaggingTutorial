{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction to Ensemble methods and Bagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Bagging is an <span style='background :yellow' > ensemble method </span>used in machine learning that can be applied to almost any <span style='background :yellow' >base learner </span> model. Generally speaking, it <span style='background :yellow' > lowers the variance of the model while maintains the low bias of the model</span>. <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; You may see a lot of new terminologies here that seems overwhelming. No worries, we will go over these concepts one by one. So far all you need to know it that bagging helps us develop a more reliable model. In this code assignment we will be starting from scratch and build up these concepts with yout knowledge in EE16A EE16B and CS61B.  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Here is the <span style='background :yellow' > outline</span> of the assignment. If you are already familiar with some concept, you can feel free to skim through the part.  \n",
    "1. Machine learning Basics recap <br>\n",
    "    1.1 Linear regression <br>\n",
    "    1.2 Polynomial regression <br>\n",
    "    1.3 Overfitting<br>\n",
    "2. Bias and Variance <br>\n",
    "    2.1 Bagging: an experiment <br>\n",
    "    2.2 Bagging theory <br>\n",
    "3. Bootstrap <br>\n",
    "4. K-fold cross-validation <br>\n",
    "5. Decision Tree <br>\n",
    "    5.1 Overfitting <br>\n",
    "    5.2 Bagged trees <br>\n",
    "    5.3 Random forest <br>\n",
    "    \n",
    "Here we want you to assess your familiarity with EE16A/B ML. If you feel confident doing linear and polynomial regression and are familiar with ideas of overfitting, you can skim through part A. If you want to have a solid background before going further, take the time to go through these code work.  \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. EE16A ML Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Here are some useful machine learning libraries we want you to import. <span style='background :yellow' >Sklearn </span> for machine learning deploymeny, <span style='background :yellow' >Numpy </span> (often seen as np.xxx ) for efficient matrix related calculation, <span style='background :yellow' >Matplotlib\n",
    "</span>(often seen as plt.xxx) for visualization. If you want to get deeper in ML, better get familiar with these libraries :))\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in /Users/chenyuecai/anaconda3/lib/python3.7/site-packages (0.18.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /Users/chenyuecai/anaconda3/lib/python3.7/site-packages (from mlxtend) (1.4.1)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /Users/chenyuecai/anaconda3/lib/python3.7/site-packages (from mlxtend) (3.1.3)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /Users/chenyuecai/anaconda3/lib/python3.7/site-packages (from mlxtend) (1.18.1)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /Users/chenyuecai/anaconda3/lib/python3.7/site-packages (from mlxtend) (0.14.1)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Users/chenyuecai/anaconda3/lib/python3.7/site-packages (from mlxtend) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /Users/chenyuecai/anaconda3/lib/python3.7/site-packages (from mlxtend) (45.1.0.post20200127)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in /Users/chenyuecai/anaconda3/lib/python3.7/site-packages (from mlxtend) (0.22.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/chenyuecai/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/chenyuecai/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/chenyuecai/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/chenyuecai/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.0.0->mlxtend) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/chenyuecai/anaconda3/lib/python3.7/site-packages (from pandas>=0.24.2->mlxtend) (2019.3)\n",
      "Requirement already satisfied: six in /Users/chenyuecai/anaconda3/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.3 is available.\n",
      "You should consider upgrading via the '/Users/chenyuecai/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sklearn.svm as svm\n",
    "import sklearn.datasets as dt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import sys\n",
    "!{sys.executable} -m pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "\n",
    "In EE16A, we have learned about the basics of machine learning: identifying the problem of classification, estimation, prediction and clustering, mastering some linear algebra techniques to solve machine learning problem, eg. least square, optimizing a loss function. (For a quick 16A ML recap please go to https://inst.eecs.berkeley.edu/~ee16a/fa19/lecture/2019-11-12_11A.pdf ) Let's start with a set of problem that we are all familiar with in EE16A, \"the line of best fit\" problems. \n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Linear regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "In this simple exercise, we will be dealing with a toy example that helps you recap the setting of linear regression. You will also be able to bridge the gap between linear algebra and the larger setting of machine learning problem. Let's say that we are given a set of peerfectly linearly correlated data and we would like to figure out the exact formulation of their relations. We learned in the EE16A that we could formulate the problem as a least square problem and find its solution.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "First we will implement a simple function that generates linear dataa dn we visualize the data by matplotlib. Remember it's alawys a good idea to visualize data so that you can get a better idea of it. Also remember to add legend so that you will not forget what these lines are for.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_linear_data(w , b):\n",
    "    xs, ys = [], []\n",
    "    for i in range(100):\n",
    "        x = 100 * random.random() - 50\n",
    "        xs.append(x)\n",
    "        ys.append(w * x + b)\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ff4d47a8940>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_linear, y_linear = gen_linear_data(1.5 , 30)\n",
    "plt.scatter(x_linear,y_linear, label = 'Linear data: w = 1.5 b = 30')\n",
    "plt.legend(loc = 'upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "\n",
    "Here we know $X$ and $y$ . We can then formulate the problem as $$Ax = b$$  \n",
    "$$A = \\begin{bmatrix}\n",
    "x_1 & 1 \\\\\n",
    "x_2 & 1 \\\\\n",
    "x_3 & 1 \\\\\n",
    "\\vdots  & \\vdots \\\\\n",
    "x_m & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "$$b = \\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots\\\\\n",
    "y_m\n",
    "\\end{bmatrix}$$ \n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "<b>Q1:</b> Derive the <span style='background :yellow' >algebric solution</span> to the Least square problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "<b>Q2:</b> Use <span style='background :yellow' >Numpy's Least square method </span>to code up the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "<b>Q3:</b> Describe how you would <span style='background :yellow' >do prediction </span>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: chocolate; line-height:1.5\">\n",
    "<b>Answer:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "    The previous example is set the stage for a larger set of regression problem called polynomial regression. Here we will give you some example to understand the set of regression problem better. Say our ground truth model is a second degree polynomial.  <br>\n",
    "    <br>\n",
    "<b>Q1:</b> If we used linear regression to model the problem, what problem will we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color:chocolate; line-height:1.5\">\n",
    "  <b> Answer: </b>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "<b>Q2:</b> Similar to ex1 where we generate some random linear data, generate second degree polynomial data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 100 points along a pre-defined polynomial\n",
    "# let the x range be from -3 to 0.\n",
    "def gen_poly2_data(a, b, c):\n",
    "    x_s, y_s = [], []\n",
    "    ### Start ###\n",
    "   \n",
    "    ### End ###\n",
    "    return x_s, y_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "<b>Q3:</b> Ploting the data for visualization. Refer the code in Part A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Start ###\n",
    "\n",
    "### End ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "<b>Q4:</b> Now it's the time to simulate the given polynomial data! We can use different degree of polynomial and end up with the parameters that fit the data. Implement the function <span style='background :yellow' > plot_best_fit_poly </span>which takes in x, y and the desired degree. Please look into function np.ployfit. It should help a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_best_fit_poly(x_s, y_s,deg):\n",
    "    ### START ###\n",
    "    \n",
    "    ### END ###\n",
    "    #plt.figure()\n",
    "    plt.plot(x_s, y_s,\"o\", new_x, new_y, linewidth = 3)\n",
    "    plt.legend(('polynomial data', str(deg)+'th degree approximation'),\n",
    "           loc='upper right')\n",
    "    \n",
    "    #plt.show()\n",
    "    print('this is best fitted polynomial for degree ' + str(deg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "<b> Q5: </b> Now let's visualize! try using polynomial of degree 3,5,10 to model the second degree polynomial example. What do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: chocolate; line-height:1.5\">\n",
    "<b>Answer:</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Imperfect data: the error term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "In real life, it is almost impossible to get a set of perfect gauched data. There is a thousand ways to get things wrong. In this part, let jump out of the perfect examples and step our foot in the real life scenarios with error. Here we will manually add guassian noise to our second degree data's observation y term. (Find you want to know more about <span style='background :yellow' >gaussian noise </span>check out this page https://www.wikiwand.com/en/Gaussian_function ) <br>\n",
    "<b> Q1: </b> Implement function add_noise with two parameters: mean and variance of the guassian noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(mu, var, ys):\n",
    "    errors = []\n",
    "    for i in range(len(ys)):\n",
    "        ### START ###\n",
    "     \n",
    "    ### END ###\n",
    "    return y_with_error  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "Here let the noise term centered around zero and have a variance of 0.5. Let's find the polynomial classifer on this noisy data instead. <br>\n",
    "    <b> Q2: </b> Plot out the classifier along with the data for at least 5 different degrees of polynomial and write down your observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: chocolate; line-height:1.5\">\n",
    "<b> Answer: </b >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Variance and Bias: a brief recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">\n",
    "In Week 2, we have learned about bias-variance decomposition. We learned about varince bias decomposition which consists of three terms: <span style='background :yellow' >bias of the method, variance of the method and irreducible error. </span>Let's first do a basic exericse to recap the concept. Here are have the ground true function $y = x + sin(1.5x)$ and the observed data is has an error term $ N ~(0,1) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_true_data(num):\n",
    "    x_s, y_s = [],[]\n",
    "    for i in range(num):\n",
    "        x = random.random() * 4 - 2\n",
    "        x_s.append(x)\n",
    "        y_s.append(x+ np.sin(1.5*x))\n",
    "    return x_s, y_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ff4d4f0e898>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_obs, y_true = gen_true_data(100)\n",
    "y_obs = add_noise(0,0.5, y_true)\n",
    "new_x = np.linspace(min(x_obs), max(x_obs))\n",
    "new_y = new_x + np.sin(1.5 * new_x)\n",
    "plt.plot(x_obs, y_obs,'o',new_x, new_y, linewidth = 3)\n",
    "plt.legend((\"noisy data\",\"ground truth value\"), loc = \"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\">  \n",
    "    <b> Q1:</b> Let's get 20 separate observation data bundle from the underlying function. Write down how would the each dataset be? Why would they be different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: chocolate; line-height:1.5\">\n",
    "<b>Answer:</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size: 11.5pt; color: dimgray; line-height:1.5\">  <b> Q2:</b> Implement the following function to generate a number of data bundles, store them in lists x_data, y_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_bundle_data(num):\n",
    "    x_data,y_data = [],[]\n",
    "    ### START ###\n",
    "\n",
    "    ### END ###\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bundle, y_bundle = gen_bundle_data(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size: 11.5pt; color: dimgray; line-height:1.5\">  <b> Q3:</b> Implement the following function bias_variance_demo. It should simulate each data bundle with degree 1-4 polynomial and report the following: <span style='background :yellow' >the mean classifier, the variance and bias of the result.</span> </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Simulate the data with degree 1-4 polynomial and report the variance and bias of the result\n",
    "\"\"\"\n",
    "def bias_variance_demo(degree):\n",
    "    coeffs = []\n",
    "    ### START ###\n",
    "    \n",
    "    ### END\n",
    "    mean_coeff = np.mean((coeffs),axis = 0)\n",
    "    mean_y = np.poly1d(mean_coeff)(new_x)\n",
    "    plt.plot(new_x, mean_y, linewidth = 3 , label = 'approximation')\n",
    "    plt.plot(new_x, new_y, linewidth = 3, label = 'ground truth')\n",
    "    plt.scatter(x_s, y_s,linewidths =0.5,  label = 'data')\n",
    "    plt.legend(loc = 'upper left')\n",
    "    plt.title(str(degree) + \"th degree polynomial simulation of the function x+ sin(1.5*x)\")\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "    bias = abs(sum(mean_y - new_y))\n",
    "    y_s = []\n",
    "    for i in range(20):\n",
    "        coeff = coeffs[i]\n",
    "        poly = np.poly1d(coeff)\n",
    "        y_s.append(poly(new_x))    \n",
    "    variance = np.var(y_s)\n",
    "    print(\"bias is \" + str(bias))\n",
    "    print(\"variance is \" + str(variance))\n",
    "    print(\"bias**2 + variance = \" + str(bias**2+variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> One would observe that as the degree of polynomial increases the bias is steadily decreasing yet the variance is increasing. Why is that? You should already encounter the concept of bias variance decomposition in previous weeks and in notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bagging: A preliminary experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "Indeed, different learners and model classes have different tradeoffs.  \n",
    "– large bias/small variance: few features, highly regularized, highly pruned decision trees, large-k kNN etc  \n",
    "– small bias/high variance: many features, less regularization, small-k k-NN etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "A natural questions arises as well deal with models with low bias and high variance. How can we prevent them from overfitting? From the previous part, we find that the mean model is much more smoothened out. From there, one natural intuition arises: averaging out the result of the model. In the previous example, we had 20 datasets generated from the underlying ground truth function. In real life, we only have one set of dataset and we do not have the gournd truth function. How could we generate all the datasets? We can simply use our gen_bundle_data function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "<b>Q1.</b> Generate noisy x and y data as well as testing data (30 data points) using previously defined function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Generate noisy x and y data'''\n",
    "### START HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Generate standard test data '''\n",
    "### START HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "<b>Q2.</b> Code up the function that computes regression error for bagging learner. It should take in training data, testing data, number of bagging process and the degree of polynomial as its paramters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_poly_error(x_train,y_train,degree, iteration,x_test, y_test):\n",
    "    y_pred = []\n",
    "    for i in range(iteration):\n",
    "        idx = random.sample(range(100),80)\n",
    "        train_x,train_y = np.array(x_train)[idx],np.array(y_train)[idx]\n",
    "        coefficients = np.polyfit(train_x,train_y, deg=degree)\n",
    "        poly = np.poly1d(coefficients)\n",
    "        y_pred.append(poly(x_test)) \n",
    "        \n",
    "    ### START ###\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_errors = []\n",
    "for i in range(2,20,2):\n",
    "    bagging_error = []\n",
    "    for i in range (1,30,3):\n",
    "        bagging_error.append(bagging_poly_error(x_noisy[0],y_noisy[0],16,i,x_test, y_test))\n",
    "    bagging_errors.append(bagging_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "<b>Q3.</b> Code up the function that computes regression error for polynomial learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Code up the function that computes regression error for polynomial learner'''\n",
    "def poly_error(x_train,y_train,degree, x_test, y_test):\n",
    "    train_x,train_y = np.array(x_train),np.array(y_train)\n",
    "    coefficients = np.polyfit(train_x,train_y, deg=degree)\n",
    "    poly = np.poly1d(coefficients)\n",
    "    y_pred = poly(x_test)\n",
    "    ### START ###\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_errors = []\n",
    "for i in range(2,20,2):\n",
    "    k = poly_error(x_noisy[0],y_noisy[0],16,x_test, y_test)\n",
    "    regression_errors.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "<b>Q4.</b> Now, plot out the bagging error of different iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "Comment on what you observe. You can see there is almost always certain times that bagging error is lower than regression error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bagging Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/700/1*_pfQ7Xf-BAwfQXtaBbNTEg.png\" width = 500pt>\n",
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "From the previous example, we can see that bagging can somehow avoid overfitting and get some pretty good result. From a theoretic pespective how does it really work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "When we do bagging, we first bootstrap M different samples without replacement from the population. Since we are sampling without replacement, then these samples are independent of each other. Thus we name them as $Y_1, Y_2, ..., Y_m$ independent random variables each with mean $\\mu$ and variance $\\sigma ^2$. As we average them, we are basically taking the expectation of the following terms which you will see is still $\\mu$ $$ \\frac{1}{M}  \\sum_{i=1}^{M} Y_i = \\frac{1}{M} \\times {M \\mu} = \\mu $$ Therefore after all these mumble jumbles we still get that good low bias. As for variance, let's do our calculation: $$ var(\\frac{1}{M}\\sum_{i=1}^{M} Y_i) = \\frac{1}{M^2} \\times var(\\sum_{i=1}^{M} Y_i) = \\frac{1}{M^2} \\times \\sigma^2 \\times M  = \\frac{\\sigma^2} {M} $$ As we can see the variance goes down linearly as M increases. This is the exact reason why we are able to get a better result in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "<b>Q1.</b> Now, observe the following results of applying bagging to <span style='background :yellow' >two exising dataset: the Iris Dataset and the Boston Housing Dataset</span> Here the base learner is decision tree and the bagged learned is random forest. Look into function <span style='background :yellow' > bias_variance_decomp </span >\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iris Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "<b>Q2.</b> Report the average bias of one single decision tree classifier. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mlxtend.data import iris_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = iris_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=y)\n",
    "\n",
    "\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=123)\n",
    "\n",
    "### START ###\n",
    "\n",
    "### end ###\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=123)\n",
    "bag = BaggingClassifier(base_estimator=tree,\n",
    "                        n_estimators=100,\n",
    "                        random_state=123)\n",
    "\n",
    "### START ###\n",
    "\n",
    "### END ###\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boston housing data example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> Now let's do the same thing to to Boston housing data. Only this time you will implement the train_test_splot and decision tree/ bagged decision tree yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.data import boston_housing_data\n",
    "\n",
    "\n",
    "X, y = boston_housing_data()\n",
    "### START ###\n",
    "\n",
    "### END ###\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(random_state=123)\n",
    "### START ###\n",
    "\n",
    "### END ###\n",
    "\n",
    "print('Average expected loss: %.3f' % avg_expected_loss)\n",
    "print('Average bias: %.3f' % avg_bias)\n",
    "print('Average variance: %.3f' % avg_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Extracting Feature from the original csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Admission_Predict.csv', index_col='Serial No.')\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Chance of Admit ', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = df['Chance of Admit '].values\n",
    "y_val[y_val>0.8] = 1\n",
    "y_val[y_val != 1] = 0\n",
    "df['Chance of Admit '] = y_val\n",
    "y = df['Chance of Admit ']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bootstrap\n",
    "\n",
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "If we randomly select some observations from dataset and use the sample to estimate some unknown value we want only once, the value cannot represent the real one. Instead, we can select sample multiple time, and average the value we found each time.It will make our prediction closer to the real value. That is the idea of Bootstrap. It is robustness and high efficiency because we do not need to add additional data.\n",
    "Bootstrap can also be used to create randomness in data.\n",
    "<img src=\"https://habrastorage.org/webt/n0/dg/du/n0dgduav1ygc3iylumtwjcn15mu.png\" width = 500>\n",
    "\n",
    "*Cite: Image from https://www.kaggle.com/kashnitsky/topic-5-ensembles-part-1-bagging*  \n",
    "\n",
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "Your task for this part is to use Bootstraping to find the average GRE score for students with `Chance of Admit` > 0.8.\n",
    "\n",
    "    \n",
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "    <b>Q1.</b> Before we actually start, let try to find the mean of `GRE Score` in this whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_mean = ### TODO ###\n",
    "print(real_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "<b>Q2.</b> Select 320 students randomly without replacement, and store the content in the variable called `sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ### TODO ###\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "<b>Q3.</b>\n",
    "In `sample` dataframe, find the average of GRE Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = ### TODO ###\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "<b>Q4.</b> Part (b) and (c) show the process of sampleing and finding the mean of sampled `GRE Score`. Now complete bootstrap function, so that we can run this processes multiple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(dataframe, n):\n",
    "    avg = []\n",
    "    for i in np.arange(n):\n",
    "        bootstrap_sample = ### TODO ###\n",
    "        new_avg = ### TODO ###\n",
    "        avg = ### TODO ###\n",
    "    \n",
    "    mean = np.mean(avg)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap(df, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "<b>Q5.</b> Compare with `onetime_mean`, `bootstrap_mean`, which one is closer to the `real_mean`? What can you conclude?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: chocolate; line-height:1.5\">\n",
    "Solution: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. K-Fold Cross-Validation\n",
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "In the case of insufficient dataset, we can use K-Fold Cross-Validation to maximize the use of the dataset. \n",
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "As you can see in the following image, after separate the dataset into training set and test set, we initialize k, which is the number of folds that we want to separate in the training set. Then we separate the training set into k equally sized subset by using KFold() function. Then we can train our model on each split, and get the corresponding error rate each time. Finally, we average the error, and use it as an estimate of the accuracy of the model algorithm.\n",
    "\n",
    "<img src=\"https://www.textbook.ds100.org/_images/bias_cv_5_fold_cv.jpg\" width = 600>\n",
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "*Cite: Image from:https://www.textbook.ds100.org/ch/15/bias_cv.html?highlight=cross#k-fold-cross-validation*\n",
    "    <br>\n",
    "In this part, we will use linear regression model to predicts TOEFL Score from GRE Score.\n",
    "<br>\n",
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "<b> Q1. </b>First, draw a scatter plot between `df['GRE Score']` and `df['TOEFL Score']`. Also set the title and axises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['GRE Score'], df['TOEFL Score'])\n",
    "plt.title(\"GRE vs TOEFL\")\n",
    "plt.xlabel(\"GRE Score\")\n",
    "plt.ylabel(\"TOEFL Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "<b> Q2. </b>Split the dataframe into 5 part, i.e. `numsplit` = 5, and compute `k_fold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_tr, X_te, Y_tr, Y_te = ### TODO ###\n",
    "\n",
    "numsplit = ### TODO ###\n",
    "k_fold = ### TODO ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "<b> Q3. </b> Build linear regression model and finish `rmse` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "model = ### TODO ###\n",
    "\n",
    "def rmse(y_real, y_predict):\n",
    "    ### TODO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "    <b> Q4. </b> Finish `rmse_kfold` function, and print the average rmse error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_kfold(X_tr):\n",
    "    err = []\n",
    "    for train_idx, valid_idx in k_fold.split(X_tr):\n",
    "        ### TODO ###\n",
    "        model.fit(split_X_train,split_Y_train)\n",
    "\n",
    "    train_err = rmse(split_Y_valid,model.predict(split_X_valid))\n",
    "    err.append(train_err)\n",
    "    return np.mean(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_err = rmse_kfold(X_tr)\n",
    "print(\"The rmse error by using K-Fold is\", rmse_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decision Tree and Bagging\n",
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "Now let's briefly introduce you to decision tree. Tree-based algorithms are a popular family of related non-parametric and supervised methods for both classification and regression. If you're wondering what supervised learning is, it's the type of machine learning algorithms which involve training models with data that has both input and output labels (in other words, we have data for which we know the true class or values, and can tell the algorithm what these are if it predicts incorrectly).\n",
    "\n",
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "The decision tree looks like a vague upside-down tree with a decision rule at the root, from which subsequent decision rules spread out below. \n",
    "\n",
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "Suppose we have four objects to classify: hawk, penguin, dolphin, and bear. It turns out that we can use a decision tree to correctly classify these four animals based on three features: \"has feathers?\", \"Can fly\", and \"Has finns\"\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/824/0*J2l5dvJ2jqRwGDfG.png\" width = 500>\n",
    "\n",
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "            A numerical representation of decision tree will be a threshold of a specific feature. Let's look at our data as an example. We limit our decision tree depth to 3 to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sklearn.tree\n",
    "dt = DecisionTreeClassifier(max_depth = 3)\n",
    "predY = dt.fit(X_tr, Y_tr).predict(X_te)\n",
    "predY[predY >= 0.5] = 1\n",
    "predY[predY != 1] = 0\n",
    "print(\"Accuracy of a single decision tree:\" + str(accuracy_score(Y_te, predY)))\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (3,3), dpi=300)\n",
    "axes.set_title('College Admission Decision Tree')\n",
    "_ = tree.plot_tree(dt, \n",
    "                   feature_names=X.columns,  \n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "It turns out that GPA is still the most important feature for graduate school admission, followed by TOEFL and GRE. Go bears!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Overfitting\n",
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "Overfitting: Or Why a Forest is better than One Tree\n",
    "The reason the decision tree is prone to overfitting when we don’t limit the maximum depth is because it has unlimited flexibility, meaning that it can keep growing until it has exactly one leaf node for every single observation, perfectly classifying all of them. But we have reduced the variance of the decision tree but at the cost of increasing the bias, resulting in a lower test accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth = 100) #TODO: change max_depth here\n",
    "dt = dt.fit(X_tr, Y_tr)\n",
    "predY_tr = dt.predict(X_tr)\n",
    "predY_tr[predY_tr >= 0.5] = 1\n",
    "predY_tr[predY_tr != 1] = 0\n",
    "predY = dt.predict(X_te)\n",
    "predY[predY >= 0.5] = 1\n",
    "predY[predY != 1] = 0\n",
    "print(\"Training accuracy:\" + str(accuracy_score(Y_tr, predY_tr)))\n",
    "print(\"Testing accuracy:\" + str(accuracy_score(Y_te, predY)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "Q. Try out on the different max depth of decision tree and report when the accuracy reaches 1 for training data and when the test accuracy reaches maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: chocolate; line-height:1.5\">\n",
    "   Solution: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Bagging them together\n",
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "Intuitively, a single tree may not be clever enough to classify our master school students. Now our goal is to implement a model that bags results of multiple decision trees and explore whether bagging is actually going to improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "class BaggedTrees:\n",
    "\n",
    "    def __init__(self, params=None, n=200):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        self.params = params\n",
    "        self.n = n\n",
    "        self.decision_trees = [\n",
    "            DecisionTreeClassifier(max_depth = 3, random_state = i, **self.params)\n",
    "            for i in range(self.n)\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n):\n",
    "            #change the following line with bootstrapping\n",
    "            self.decision_trees[i].fit(X_tr, Y_tr)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        yhat = [self.decision_trees[i].predict(X) for i in range(self.n)]\n",
    "        # TODO: compute yhat_avg for BaggedTrees\n",
    "        # HINT: take the average and predict results, note that our prediction only consists of 0 and 1\n",
    "        ### start code ###\n",
    "        \n",
    "        ### end code ###\n",
    "        return yhat_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt = BaggedTrees()\n",
    "predY = bt.fit(X_tr, Y_tr).predict(X_te)\n",
    "predY[predY >= 0.5] = 1\n",
    "predY[predY != 1] = 0\n",
    "print(accuracy_score(Y_te, predY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "As you can see the accuracy of testing data is getting better after averaging the result from 100 decision trees or any number of decision trees (change n)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Random Forest\n",
    "\n",
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "You just implemented a random forest by yourself! Congratulations. Random Forest is a tree-based machine learning algorithm that leverages the power of multiple decision trees for making decisions. As the name suggests, it is a “forest” of trees!\n",
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "But why do we call it a “random” forest? That’s because it is a forest of randomly created decision trees. As you can see from the skeleton code above -- we set the random state for each decision tree and thus each node in the decision tree works on a random subset of features to calculate the output. The random forest then combines the output of individual decision trees to generate the final output.\n",
    "<span style=\"font-family:Georgia; font-size:11.5pt; color: dimgray; line-height:1.5\"> \n",
    "And that is our bagged trees. Let's see the result of random forest implemented by sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(max_depth = 3,n_estimators = 50,random_state=1)\n",
    "predY = rf.fit(X_tr, Y_tr).predict(X_te)\n",
    "predY[predY >= 0.5] = 1\n",
    "predY[predY != 1] = 0\n",
    "print(accuracy_score(Y_te, predY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://miro.medium.com/max/500/1*10t9S7xvWE5Z3NEZrmHG2w.jpeg\">\n",
    "<center>This is bagging, try to get the intuition here.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "https://blog.paperspace.com/decision-trees/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://becominghuman.ai/ensemble-learning-bagging-and-boosting-d20f38be9b1e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/kashnitsky/topic-5-ensembles-part-1-bagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.textbook.ds100.org/ch/15/bias_cv.html?highlight=cross#k-fold-cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
